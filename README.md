## 1. Predator-Prey Simulation
- Simulation of predators trying to catch prey using cooperative strategy.


This code: 
âœ… Implements cooperative predators
âœ… Uses Manhattan distance for pathfinding
âœ… Uses communication between predators
âœ… Real-time updates with matplotlib

ğŸ” Features Added:
âœ… Cooperative behavior for predators
âœ… Prey attempts to evade capture strategically
âœ… Real-time updates and visualization
âœ… Dynamic pathfinding using Manhattan distance
âœ… Event-based termination (capture or survival)

### How to Run
python multi-agent-predator-prey/predator_prey.py


## 2. Predator-Prey with Q-Learning
- Multi-agent reinforcement learning using Q-Learning for predator-prey simulation.

ğŸ”¥ Why RL?
âœ… Reinforcement Learning allows agents to learn from experience instead of relying on pre-coded strategies.
âœ… It introduces dynamic, adaptive behavior as agents improve based on feedback.
âœ… Multi-Agent RL (MARL) is a hot field in robotics, game AI, and autonomous systems.

ğŸ† New Features to Add:
âœ… Q-Learning for decision-making (agents learn from rewards and penalties).
âœ… Shared state for cooperative learning among predators.
âœ… Exploration vs. Exploitation â€“ Encourage agents to explore the environment early and exploit learned knowledge later.
âœ… Custom Reward System â€“ Reward successful capture, penalize collisions.
âœ… Training Phase â€“ Run a training loop for agents to learn better strategies over time.

### How to Run
python multi-agent-rl/predator_prey_qlearning.py

## 3. Advanced Multi-Agent Predator-Prey Simulation with DQN
- Multi-agent system using Deep Q-Learning (DQN) with communication, flocking, and adaptive prey behavior.

âœ… Why DQN?
Handle larger state-action spaces using neural networks.
Handle continuous state spaces better than tabular methods.

âœ… Why Flocking?
Flocking creates realistic behavior based on three rules:
Alignment â€“ Move in the same direction as nearby agents.
Cohesion â€“ Move toward the average position of nearby agents.
Separation â€“ Avoid getting too close to other agents.

âœ… Why Adaptive Prey?
Make prey smarter by using pathfinding and grouping.
Prey can also learn using SARSA or DQN.

âœ… Why Team Rewards?
Encourage cooperation over selfish behavior.
Higher reward when multiple predators contribute to catching prey.

ğŸš€ Next-Level Enhancements:
âœ… 1. SARSA-Based Prey Learning
Prey will no longer move randomly â€” it will learn using SARSA (State-Action-Reward-State-Action).
This introduces competition between learning predators and learning prey.
âœ… 2. Reward Balancing for Cooperation vs Individual Reward
Predators will receive higher rewards for teamwork.
Adjust reward scaling to favor cooperative behavior.
âœ… 3. Heatmap Visualization
Show predator and prey movements over time using a heatmap.
Track areas with higher activity and hunting success rates.
âœ… 4. TensorBoard Logging
Track average reward, episode length, and prey caught over time.
Monitor training stability and learning rate.

ğŸš€ Next-Level Enhancements:
âœ… 1. Competitive Learning Between Predators
Introduce a reward hierarchy:
Predators get individual rewards for catching prey.
Add a bonus reward for teamwork (if more than one predator participates).
Add a penalty for interfering with another predatorâ€™s catch.
âœ… 2. Introduce Predator "Roles"
Assign different behaviors to predators:
Chasers â€“ Aggressive, high-reward for capture.
Blockers â€“ Defend territory and reduce prey escape options.
Scouts â€“ Locate prey and communicate positions to others.
âœ… 3. Multi-Agent Cooperation and Competition
Add communication between predators to form temporary alliances.
Include probabilistic betrayal â€” predators may defect for higher individual rewards.
âœ… 4. Adaptive Prey Strategy
Prey should adapt dynamically:
Form groups when threatened.
Introduce a leader-following mechanism for coordinated movement.
Prey can start â€œdecoyingâ€ to mislead predators.
âœ… 5. Memory for Prey and Predators
Implement a short-term memory:
Agents remember past actions and outcomes.
Use this to improve decision-making.
âœ… 6. Reinforcement Learning with PPO (Proximal Policy Optimization)
Upgrade from DQN to PPO for more stable learning:
PPO handles continuous action spaces better.
PPO helps reduce overfitting and instability.
âœ… 7. Add Logging to TensorBoard
Log the following metrics:
Average reward per episode
Average number of steps to capture
Success rate over episodes

âœ… The code is now upgraded with elite-level MAS complexity:

ğŸ”¥ Predator Roles â€“ Predators now take on Chaser, Blocker, and Scout roles.
ğŸ”¥ Competitive and Cooperative Rewards â€“ Higher rewards for team play; penalties for selfish behavior.
ğŸ”¥ Multi-Agent Communication â€“ Predators share information to improve hunting strategy.
ğŸ”¥ Adaptive Prey Strategy â€“ Prey actively tries to mislead predators.
ğŸ”¥ Short-Term Memory â€“ Agents now adapt based on recent experiences.

### Final Version Features 
âœ… PPO-based learning  
âœ… Predator roles: Chaser, Blocker, Scout  
âœ… Prey roles: Leader, Decoy  
âœ… Real-time communication between predators  
âœ… Dynamic reward system  
âœ… Heatmap visualization of movement  

### How to Run
python multi-agent-dqn/multi_agent_predator_prey_dqn.py